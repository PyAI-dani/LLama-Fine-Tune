{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.14","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"nvidiaTeslaT4","dataSources":[],"dockerImageVersionId":30805,"isInternetEnabled":false,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"%%capture\n%pip install accelerate peft bitsandbytes transformers trl","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-18T18:16:23.921658Z","iopub.execute_input":"2024-12-18T18:16:23.922336Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"%%capture\n!pip3 install torch torchvision torchaudio --index-url https://download.pytorch.org/whl/cu118","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"!nvcc -V","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"!nvidia-smi","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import os\nimport torch\nfrom datasets import load_dataset\nfrom transformers import (\n    AutoModelForCausalLM,\n    AutoTokenizer,\n    BitsAndBytesConfig,\n    TrainingArguments,\n    pipeline,\n    logging,\n)\nfrom peft import LoraConfig\nfrom trl import SFTTrainer","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Model from Hugging Face hub\nbase_model = \"NousResearch/Llama-2-7b-chat-hf\"\n\n# New instruction dataset\nguanaco_dataset = \"mlabonne/guanaco-llama2-1k\"\n\n# Fine-tuned model\nnew_model = \"llama-2-7b-chat-guanaco\"","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"dataset = load_dataset(guanaco_dataset, split=\"train\")","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"dataset['text'][0]","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"0\"","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"compute_dtype = getattr(torch, \"float16\")\n\nquant_config = BitsAndBytesConfig(\n    load_in_4bit=True,\n    bnb_4bit_quant_type=\"nf4\",\n    bnb_4bit_compute_dtype=compute_dtype,\n    bnb_4bit_use_double_quant=False,\n)","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"model = AutoModelForCausalLM.from_pretrained(\n    base_model,\n    quantization_config=quant_config,\n    device_map={\"\": 0},)\nmodel.config.use_cache = False\nmodel.config.pretraining_tp = 1","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"tokenizer = AutoTokenizer.from_pretrained(base_model, trust_remote_code=True)\ntokenizer.pad_token = tokenizer.eos_token\ntokenizer.padding_side = \"right\"","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"peft_params = LoraConfig(\n    lora_alpha=16,\n    lora_dropout=0.1,\n    r=64,\n    bias=\"none\",\n    task_type=\"CAUSAL_LM\",\n)","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"training_params = TrainingArguments(\n    output_dir=\"./results\",\n    num_train_epochs=1,\n    per_device_train_batch_size=1,\n    gradient_accumulation_steps=1,\n    optim=\"paged_adamw_32bit\",\n    save_steps=25,\n    logging_steps=25,\n    learning_rate=2e-4,\n    weight_decay=0.001,\n    fp16=True,\n    max_grad_norm=0.3,\n    max_steps=-1,\n    warmup_ratio=0.03,\n    group_by_length=True,\n    lr_scheduler_type=\"constant\",\n    report_to=\"tensorboard\"\n)","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"trainer = SFTTrainer(\n    model=model,\n    train_dataset=dataset,\n    peft_config=peft_params,\n    tokenizer=tokenizer,\n    args=training_params,\n)","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"!nvidia-smi","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"trainer.train()","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"trainer.save_model('fine_tuned_model')","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"1\"","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"1\"  # به شماره GPU صحیح تغییر دهید\n\nimport torch\nfrom transformers import AutoModelForCausalLM, AutoTokenizer, pipeline\n\n# مسیر محلی به مدل فاین‌تیون‌شده\nfine_tuned_model_path = \"fine_tuned_model\"\n\n# لود کردن مدل\nmodel = AutoModelForCausalLM.from_pretrained(\n    fine_tuned_model_path,\n    # device_map=\"auto\",\n    torch_dtype=torch.float16,\n    trust_remote_code=True,\n    offload_folder=\"./offload\",\n    load_in_4bit=True\n)\n\n\n# لود کردن توکنایزر\ntokenizer = AutoTokenizer.from_pretrained(\n    \"fine_tuned_model\",\n    trust_remote_code=True\n)\n\n# آماده کردن مدل برای پردازش ورودی\npipe = pipeline(\n    task=\"text-generation\",\n    model=model,\n    tokenizer=tokenizer,\n    max_length=50,\n    temperature=0.7,\n    top_k=10,\n    top_p=0.8\n)\n\n# آماده برای پرسیدن سؤال\nwhile True:\n    user_input = input(\"پرسش خود را وارد کنید (برای خروج 'exit' را تایپ کنید): \")\n    if user_input.lower() == \"exit\":\n        print(\"خروج از برنامه.\")\n        break\n    prompt = f\"<s>[INST] {user_input} [/INST]\"\n    response = pipe(prompt)\n    print(\"پاسخ مدل:\", response[0]['generated_text'])","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# tokenizer = AutoTokenizer.from_pretrained(\"fine_tuned_model\")\n\n# from peft import PeftModel\n\n# model = AutoModelForCausalLM.from_pretrained(\n#     \"fine_tuned_model\",\n#     torch_dtype=torch.float16,\n#     # device_map=\n# )\n\n# # model.resize_token_embeddings(len(tokenizer), pad_to_multiple_of=1)\n# model = PeftModel.from_pretrained(model, \"fine_tuned_model\")\n# # model = model.merge_and_unload()","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"logging.set_verbosity(logging.CRITICAL)\n\nprompt = \"hello, how are you?\"\npipe = pipeline(task=\"text-generation\", model=model, tokenizer=tokenizer, max_length=20)\nresult = pipe(f\"<s>[INST] {prompt} [/INST]\")\nprint(result[0]['generated_text'])","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"print('hello, welcome to our llama model, if you want to exit the app enter: exit')\nwhile True:\n    prompt = str(input('enter your prompt: '))\n    \n    pipe = pipeline(task=\"text-generation\", model=model, tokenizer=tokenizer, max_length=200)\n    result = pipe(f\"<s>[INST] {prompt} [/INST]\")\n    print(result[0]['generated_text'])\n\n    if prompt == \"exit\":\n        break","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null}]}